{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed58759",
   "metadata": {},
   "source": [
    "# Data Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ceb1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# * Import Required Libraries\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn import datasets => iris = datasets.load_iris()\n",
    "from sklearn.datasets import load_iris # => iris = load_iris()\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c09bf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# #. Load the Iris Dataset\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m      4\u001b[39m df = load_iris()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = pd.DataFrame(\u001b[43miris\u001b[49m.data, columns=iris.feature_names)\n\u001b[32m      6\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m'\u001b[39m] = iris.target\n\u001b[32m      7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m'\u001b[39m].map({\u001b[32m0\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msetosa\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mversicolor\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mvirginica\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[31mNameError\u001b[39m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# #. Load the Iris Dataset\n",
    "# ===============================\n",
    "df = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Normalization & Standardization\n",
    "# ===============================\n",
    "minmax_scaler = MinMaxScaler()\n",
    "zscore_scaler = StandardScaler()\n",
    "\n",
    "df_minmax = pd.DataFrame(minmax_scaler.fit_transform(df.iloc[:, :-1]), columns=df.columns[:-1])\n",
    "df_zscore = pd.DataFrame(zscore_scaler.fit_transform(df.iloc[:, :-1]), columns=df.columns[:-1])\n",
    "\n",
    "df_minmax['species'] = df['species']\n",
    "df_zscore['species'] = df['species']\n",
    "\n",
    "print(\"Min-Max Normalized Data:\")\n",
    "display(df_minmax.head())\n",
    "\n",
    "print(\"\\nZ-Score Standardized Data:\")\n",
    "display(df_zscore.head())\n",
    "# Compare original, min-max, and z-score distributions for one feature\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sns.histplot(df['sepal length (cm)'], kde=True, ax=ax[0], color=\"skyblue\")\n",
    "ax[0].set_title(\"Original Sepal Length\")\n",
    "\n",
    "sns.histplot(df_minmax['sepal length (cm)'], kde=True, ax=ax[1], color=\"orange\")\n",
    "ax[1].set_title(\"Min-Max Normalized\")\n",
    "\n",
    "sns.histplot(df_zscore['sepal length (cm)'], kde=True, ax=ax[2], color=\"green\")\n",
    "ax[2].set_title(\"Z-Score Standardized\")\n",
    "\n",
    "plt.suptitle(\"Normalization and Standardization Comparison\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98066a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. K-Means Clustering (1D & 2D)\n",
    "# ===============================\n",
    "# 1D Clustering (Sepal Length)\n",
    "X1 = df[['sepal length (cm)']]\n",
    "kmeans_1d = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster_1D'] = kmeans_1d.fit_predict(X1)\n",
    "\n",
    "plt.scatter(X1, [0.05] * len(X1), c=df['Cluster_1D'], cmap='viridis')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.title('1D K-Means Clustering')\n",
    "plt.show()\n",
    "\n",
    "# 2D Clustering (Sepal Length & Width)\n",
    "X2 = df[['sepal length (cm)', 'sepal width (cm)']]\n",
    "kmeans_2d = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster_2D'] = kmeans_2d.fit_predict(X2)\n",
    "\n",
    "plt.scatter(X2.iloc[:, 0], X2.iloc[:, 1], c=df['Cluster_2D'], cmap='rainbow')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('2D K-Means Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e88677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4. Classification â€” KNN, NB, Decision Tree\n",
    "# ===============================\n",
    "X = df.iloc[:, :-3]\n",
    "y = df['species']\n",
    "\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=['setosa', 'versicolor', 'virginica'])\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1] + cm[0][2]\n",
    "    FN = cm[1][0] + cm[2][0]\n",
    "    TP = cm[1][1] + cm[2][2]\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (FN + TP)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'F1-Score': f1_score(y_test, y_pred, average='macro'),\n",
    "        'TPR': TPR, 'FPR': FPR, 'TNR': TNR, 'FNR': FNR\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. Apriori Algorithm\n",
    "# ===============================\n",
    "# Convert dataset to transaction-like format\n",
    "transactions = []\n",
    "for _, row in df.iterrows():\n",
    "    transaction = [\n",
    "        f\"SepalLength={round(row['sepal length (cm)'], 1)}\",\n",
    "        f\"SepalWidth={round(row['sepal width (cm)'], 1)}\",\n",
    "        f\"PetalLength={round(row['petal length (cm)'], 1)}\",\n",
    "        f\"PetalWidth={round(row['petal width (cm)'], 1)}\",\n",
    "        f\"Class={row['species']}\"\n",
    "    ]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Encode transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_apriori = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Run Apriori with two different thresholds\n",
    "freq_items_1 = apriori(df_apriori, min_support=0.5, use_colnames=True)\n",
    "rules_1 = association_rules(freq_items_1, metric='confidence', min_threshold=0.75)\n",
    "print(\"Apriori (Support=50%, Confidence=75%)\")\n",
    "display(rules_1[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "freq_items_2 = apriori(df_apriori, min_support=0.6, use_colnames=True)\n",
    "rules_2 = association_rules(freq_items_2, metric='confidence', min_threshold=0.6)\n",
    "print(\"\\nApriori (Support=60%, Confidence=60%)\")\n",
    "display(rules_2[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
